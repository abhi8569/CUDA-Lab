{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os as os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import randperm\n",
    "from torch._utils import _accumulate\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms.functional as F\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from skimage import io, transform\n",
    "import sklearn.metrics as skm\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bounding_box(bbox):\n",
    "    print(\"*-*-*-*-*-*-*-* Bounding Box *-*-*-*-*-*-*-*\")\n",
    "    print(\"Xmin : \",bbox[0][0],\" Ymin : \",bbox[0][1])\n",
    "    print(\"Xmax : \",bbox[1,0],\" Ymax : \",bbox[1,1])\n",
    "    print(\"Xcent : \",bbox[2,0],\" Ycent : \",bbox[2,1])\n",
    "    print(\"Xlen : \",bbox[3,0],\" Ylen : \",bbox[3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, data_dict): \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                image = data_dict[key]\n",
    "                image = F.to_tensor(image)\n",
    "                transformed_dict[key] = image\n",
    "            else:\n",
    "                dtls = torch.FloatTensor(data_dict[key])\n",
    "                transformed_dict[key] = dtls\n",
    "                \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        \n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Rescale Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #image.show()\n",
    "        #print(\"Old iMage dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if isinstance(self.output_size, int):        \n",
    "            \n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "                \n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                img = F.resize(image, (new_h, new_w))\n",
    "                \n",
    "                #img.show()\n",
    "                #print(\"new iMage dimension : \",new_w,\"x\",new_h)\n",
    "                \n",
    "                transformed_dict[key] = img\n",
    "            else:\n",
    "                bounding_box = data_dict[key]\n",
    "                \n",
    "                #print(bounding_box)\n",
    "                \n",
    "                for i, dtls in enumerate(bounding_box):\n",
    "    \n",
    "                    #print(\"=====================Old Bounding Box================\")\n",
    "                    #print_bounding_box(dtls)\n",
    "            \n",
    "                    bounding_box[i][0] = np.round(dtls[0] * np.array([new_w / w, new_h / h]), 0)\n",
    "                    bounding_box[i][1] = np.round(dtls[1] * np.array([new_w / w, new_h / h]), 0)\n",
    "                    bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                    bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                    \n",
    "                    #print(\"=====================New Bounding Box================\")\n",
    "                    #print_bounding_box(bounding_box[i])\n",
    "\n",
    "                transformed_dict[key] = bounding_box\n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVerticalFlip(object):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        \n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Verticle Flip Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #print(\"Flipped image dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if random.random() < self.p:\n",
    "            \n",
    "            for key in data_dict.keys():\n",
    "                \n",
    "                if key == 'image':\n",
    "                    image = F.vflip(image)\n",
    "                    \n",
    "                    #image.show()\n",
    "                    \n",
    "                    transformed_dict[key] = image\n",
    "                else:\n",
    "                    bounding_box = data_dict[key]\n",
    "                    \n",
    "                    #print(bounding_box)\n",
    "                    \n",
    "                    for i, dtls in enumerate(bounding_box):\n",
    "                        \n",
    "                        #print(\"=====================Old Bounding Box================\")\n",
    "                        #print_bounding_box(dtls)\n",
    "                        \n",
    "                        if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
    "                            bounding_box[i][0] = np.array([dtls[0, 0], h - dtls[1, 1]])\n",
    "                            bounding_box[i][1] = np.array([dtls[1, 0], bounding_box[i][0, 1] + dtls[3, 1] ])\n",
    "                            bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                            bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                            \n",
    "                            #print(\"=====================New Bounding Box================\")\n",
    "                            #print_bounding_box(bounding_box[i])\n",
    "                            \n",
    "                    transformed_dict[key] = bounding_box\n",
    "        else:\n",
    "            return data_dict\n",
    "        \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip(object):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        \n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Horizontal Flip Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #print(\"Flipped image dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if random.random() < self.p:\n",
    "            \n",
    "            for key in data_dict.keys():\n",
    "                \n",
    "                if key == 'image':\n",
    "                    image = F.hflip(image)\n",
    "                    \n",
    "                    #image.show()\n",
    "                    \n",
    "                    transformed_dict[key] = image\n",
    "                else:\n",
    "                    bounding_box = data_dict[key]\n",
    "                    \n",
    "                    #print(bounding_box)\n",
    "                    \n",
    "                    for i, dtls in enumerate(bounding_box):\n",
    "                        \n",
    "                        #print(\"=====================Old Bounding Box================\")\n",
    "                        #print_bounding_box(dtls)\n",
    "                        \n",
    "                        if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
    "                            bounding_box[i][0] = np.array([w - (dtls[0, 0] + dtls[3, 0]), dtls[0, 1]])\n",
    "                            bounding_box[i][1] = np.array([bounding_box[i][0, 0] + dtls[3, 0], dtls[1, 1] ])\n",
    "                            bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                            bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                            \n",
    "                            #print(\"=====================New Bounding Box================\")\n",
    "                            #print_bounding_box(bounding_box[i])\n",
    "                            \n",
    "                    transformed_dict[key] = bounding_box\n",
    "        else:\n",
    "            return data_dict\n",
    "        \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "    \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        bounding_box = dict()\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                image = data_dict[key]\n",
    "                bounding_box[key] = F.normalize(image, self.mean, self.std)\n",
    "            else:\n",
    "                bounding_box[key] = data_dict[key]\n",
    "                \n",
    "        return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(object):\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert callable(lambd), repr(type(lambd).__name__) + \\\n",
    "            \" object is not callable\"\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorJitter(object):\n",
    "\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = self._check_input(brightness, 'brightness')\n",
    "        self.contrast = self._check_input(contrast, 'contrast')\n",
    "        self.saturation = self._check_input(saturation, 'saturation')\n",
    "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
    "                                     clip_first_on_zero=False)\n",
    "\n",
    "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
    "        if isinstance(value, numbers.Number):\n",
    "            if value < 0:\n",
    "                raise ValueError(\n",
    "                    \"If {} is a single number, it must be non negative.\".format(name))\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\n",
    "                    \"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n",
    "\n",
    "        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n",
    "        # or (0., 0.) for hue, do nothing\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness, contrast, saturation, hue):\n",
    "        \"\"\"Get a randomized transform to be applied on image.\n",
    "\n",
    "        Arguments are same as that of __init__.\n",
    "\n",
    "        Returns:\n",
    "            Transform which randomly adjusts brightness, contrast and\n",
    "            saturation in a random order.\n",
    "        \"\"\"\n",
    "        tforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast is not None:\n",
    "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        if saturation is not None:\n",
    "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
    "\n",
    "        if hue is not None:\n",
    "            hue_factor = random.uniform(hue[0], hue[1])\n",
    "            tforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
    "\n",
    "        random.shuffle(tforms)\n",
    "        transform = transforms.Compose(tforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample (List): List of Input image and bounding box\n",
    "\n",
    "        Returns:\n",
    "            List: Color jittered image and original bounding box.\n",
    "        \"\"\"\n",
    "        items = dict()\n",
    "        for key in sample.keys():\n",
    "            if key == 'image':\n",
    "                image = sample[key]\n",
    "                transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "                items[key] =  transform(image)\n",
    "            else:\n",
    "                items[key] = sample[key]\n",
    "        return items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
