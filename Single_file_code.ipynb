{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os as os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numbers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import randperm\n",
    "from torch._utils import _accumulate\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms.functional as F\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from skimage import io, transform\n",
    "import sklearn.metrics as skm\n",
    "from scipy.spatial import distance\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlobTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform = None):\n",
    "        \n",
    "        self.path = path \n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.xml_path = \"\"\n",
    "        for file in os.listdir(path):\n",
    "            #if file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "            if(file.endswith(\".png\")):\n",
    "                self.xml_path = file.replace(\".png\",\".xml\")\n",
    "            elif(file.endswith(\".jpg\")):\n",
    "                self.xml_path = file.replace(\".jpg\",\".xml\")\n",
    "            elif(file.endswith(\".jpeg\")):\n",
    "                self.xml_path = file.replace(\".jpeg\",\".xml\")\n",
    "                \n",
    "            if file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "                #print(os.path.join(self.path, self.xml_path))\n",
    "                if os.path.exists(os.path.join(self.path, self.xml_path)):\n",
    "                    self.file_list.append(file)\n",
    "                \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = os.path.join(self.path, self.file_list[index])\n",
    "        #print(self.file_list[index])\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        if(path.endswith(\".png\")):\n",
    "            annotation = ET.parse(path.replace(\".png\",\".xml\")).getroot()\n",
    "        elif(path.endswith(\".jpg\")):\n",
    "            annotation = ET.parse(path.replace(\".jpg\",\".xml\")).getroot()\n",
    "        elif(path.endswith(\".jpeg\")):\n",
    "            annotation = ET.parse(path.replace(\".jpeg\",\".xml\")).getroot()\n",
    "            \n",
    "        anno_detail = []\n",
    "        \n",
    "        for element in annotation.findall(\"object\"):\n",
    "            anno_bound_box = element.find(\"bndbox\")\n",
    "            anno_detail.append(get_bounding_box(anno_bound_box))\n",
    "        \n",
    "        data_dict = {\"image\" : image, \"data\" : anno_detail}\n",
    "        \n",
    "        if(self.transform):\n",
    "            \n",
    "            if(type(self.transform) is not list):\n",
    "                self.transform = [self.transform]\n",
    "            \n",
    "            for idx in range(len(self.transform)):\n",
    "                data_dict = self.transform[idx](data_dict)\n",
    "        \n",
    "        image_size = data_dict[\"image\"].shape\n",
    "        \n",
    "        heatmap_placeholder = torch.zeros([3, int(image_size[1]/4), int(image_size[2]/4)])\n",
    "        \n",
    "        object_counter = 0\n",
    "        \n",
    "        for obj in annotation.findall(\"object\"):\n",
    "            \n",
    "            label = obj.find(\"name\").text\n",
    "            \n",
    "            image_heatmap = heatmap(data_dict[\"data\"][object_counter],(image_size[1],image_size[2]))\n",
    "            \n",
    "            if(label == \"ball\"):\n",
    "                heatmap_placeholder[0] += image_heatmap\n",
    "            elif (label == \"goalpost\"):\n",
    "                heatmap_placeholder[1] += image_heatmap \n",
    "            elif (label == \"robot\"):\n",
    "                heatmap_placeholder[2] += image_heatmap \n",
    "            \n",
    "            object_counter+=1\n",
    "            \n",
    "        dataset = {'image': data_dict['image'], 'heatmap': heatmap_placeholder}\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlobTestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform = None):\n",
    "        \n",
    "        self.path = path \n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.xml_path = \"\"\n",
    "        for file in os.listdir(path):\n",
    "            #if file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "            if(file.endswith(\".png\")):\n",
    "                self.xml_path = file.replace(\".png\",\".xml\")\n",
    "            elif(file.endswith(\".jpg\")):\n",
    "                self.xml_path = file.replace(\".jpg\",\".xml\")\n",
    "            elif(file.endswith(\".jpeg\")):\n",
    "                self.xml_path = file.replace(\".jpeg\",\".xml\")\n",
    "                \n",
    "            if file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "                #print(os.path.join(self.path, self.xml_path))\n",
    "                if os.path.exists(os.path.join(self.path, self.xml_path)):\n",
    "                    self.file_list.append(file)\n",
    "                \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = os.path.join(self.path, self.file_list[index])\n",
    "        #print(self.file_list[index])\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        if(path.endswith(\".png\")):\n",
    "            annotation = ET.parse(path.replace(\".png\",\".xml\")).getroot()\n",
    "        elif(path.endswith(\".jpg\")):\n",
    "            annotation = ET.parse(path.replace(\".jpg\",\".xml\")).getroot()\n",
    "        elif(path.endswith(\".jpeg\")):\n",
    "            annotation = ET.parse(path.replace(\".jpeg\",\".xml\")).getroot()\n",
    "            \n",
    "        ball_detail = []\n",
    "        goalpost_detail = []\n",
    "        robot_detail = []\n",
    "        \n",
    "        for element in annotation.findall(\"object\"):\n",
    "            \n",
    "            anno_bound_box = element.find(\"bndbox\")\n",
    "            label = element.find(\"name\").text\n",
    "            \n",
    "            if(label == \"ball\"):\n",
    "                ball_detail.append(get_bounding_box(anno_bound_box))\n",
    "            \n",
    "            elif(label == \"goalpost\"):\n",
    "                goalpost_detail.append(get_bounding_box(anno_bound_box))\n",
    "                \n",
    "            elif(label == \"robot\"):\n",
    "                robot_detail.append(get_bounding_box(anno_bound_box))\n",
    "                \n",
    "        dataset = {\"image\" : image,\n",
    "                   \"ball_detail\" : ball_detail,\n",
    "                   \"goalpost_detail\" : goalpost_detail,\n",
    "                   \"robot_detail\" : robot_detail}\n",
    "        \n",
    "        if(self.transform):\n",
    "            \n",
    "            if(type(self.transform) is not list):\n",
    "                self.transform = [self.transform]\n",
    "            \n",
    "            for idx in range(len(self.transform)):\n",
    "                dataset = self.transform[idx](dataset)\n",
    "                \n",
    "        return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform = None):\n",
    "        \n",
    "        self.path = path \n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.target_image = \"\"\n",
    "        self.image_path = path + \"/image/\"\n",
    "        self.target_path = path + \"/target/\"\n",
    "\n",
    "        for file in os.listdir(self.image_path):\n",
    "            if(file.endswith(\".jpg\")):\n",
    "                self.target_image = file.replace(\".jpg\",\".png\")\n",
    "                if os.path.exists(os.path.join(self.target_path, self.target_image)):\n",
    "                    self.file_list.append(file)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        file_name = self.file_list[index]\n",
    "        \n",
    "        image = torch.FloatTensor(self.load_image(self.image_path + \"/\" + file_name))\n",
    "        target = torch.FloatTensor(self.load_mask(self.target_path + \"/\" +  file_name.replace(\".jpg\",\".png\")))\n",
    "        \n",
    "        return image, target\n",
    "        \n",
    "    def load_image(self, path=None):\n",
    "        \n",
    "        raw_image = Image.open(path).convert('RGB')\n",
    "        #raw_image = np.transpose(raw_image.resize((480, 640)), (2,1,0))\n",
    "        raw_image = F.resize(raw_image, (480, 640))\n",
    "        #raw_image = F.vflip(raw_image)\n",
    "        imx_t = np.array(raw_image, dtype=np.float32)/255.0\n",
    "        imx_t = np.transpose(imx_t, (2, 0, 1))\n",
    "        return imx_t\n",
    "    \n",
    "    def load_mask(self, path=None):\n",
    "        \n",
    "        raw_image = Image.open(path)\n",
    "#         print(\"Label : -*-*-*-*-*-*-*-*-*-**-*-*-*\\n \",np.array(raw_image))\n",
    "        raw_image = F.resize(raw_image, (120, 160))\n",
    "#         print(\"Size : \",np.array(raw_image.size))\n",
    "#         print(\"Label : -*-*-*-*-*-*-*-*-*-**-*-*-*\\n \",imx_t)\n",
    "        imx_t = np.array(raw_image)\n",
    "        imx_t[imx_t == 1] = 2\n",
    "        imx_t[imx_t == 2] = 1\n",
    "        imx_t[imx_t == 3] = 2 \n",
    "        #[imx_t==i.astype(int) for i in np.unique(imx_t)]\n",
    "        imx_t = np.equal.outer(np.unique(imx_t),imx_t).view('i1')\n",
    "        imx_t = np.array(imx_t)\n",
    "#         print(\"Label : -*-*-*-*-*-*-*-*-*-**-*-*-*\\n \",np.array(raw_image))\n",
    "#         imx_t[imx_t==255] = 3\n",
    "        imx_t = np.transpose(imx_t, (0,1,2))\n",
    "        #imx_t = np.transpose(imx_t, (0, 1))\n",
    "#         imx_t = np.array(raw_image, dtype=np.float32)/255.0\n",
    "        return imx_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    def __call__(self, data_dict): \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                image = data_dict[key]\n",
    "                image = F.to_tensor(image)\n",
    "                transformed_dict[key] = image\n",
    "            else:\n",
    "                dtls = torch.FloatTensor(data_dict[key])\n",
    "                transformed_dict[key] = dtls\n",
    "                \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        \n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Rescale Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #image.show()\n",
    "        #print(\"Old iMage dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if isinstance(self.output_size, int):        \n",
    "            \n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "                \n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                img = F.resize(image, (new_h, new_w))\n",
    "                \n",
    "                #img.show()\n",
    "                #print(\"new iMage dimension : \",new_w,\"x\",new_h)\n",
    "                \n",
    "                transformed_dict[key] = img\n",
    "            else:\n",
    "                bounding_box = data_dict[key]\n",
    "                \n",
    "                #print(bounding_box)\n",
    "                \n",
    "                for i, dtls in enumerate(bounding_box):\n",
    "    \n",
    "                    #print(\"=====================Old Bounding Box================\")\n",
    "                    #print_bounding_box(dtls)\n",
    "            \n",
    "                    bounding_box[i][0] = np.round(dtls[0] * np.array([new_w / w, new_h / h]), 0)\n",
    "                    bounding_box[i][1] = np.round(dtls[1] * np.array([new_w / w, new_h / h]), 0)\n",
    "                    bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                    bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                    \n",
    "                    #print(\"=====================New Bounding Box================\")\n",
    "                    #print_bounding_box(bounding_box[i])\n",
    "\n",
    "                transformed_dict[key] = bounding_box\n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVerticalFlip(object):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        \n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Verticle Flip Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #print(\"Flipped image dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if random.random() < self.p:\n",
    "            \n",
    "            for key in data_dict.keys():\n",
    "                \n",
    "                if key == 'image':\n",
    "                    image = F.vflip(image)\n",
    "                    \n",
    "                    #image.show()\n",
    "                    \n",
    "                    transformed_dict[key] = image\n",
    "                else:\n",
    "                    bounding_box = data_dict[key]\n",
    "                    \n",
    "                    #print(bounding_box)\n",
    "                    \n",
    "                    for i, dtls in enumerate(bounding_box):\n",
    "                        \n",
    "                        #print(\"=====================Old Bounding Box================\")\n",
    "                        #print_bounding_box(dtls)\n",
    "                        \n",
    "                        if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
    "                            bounding_box[i][0] = np.array([dtls[0, 0], h - dtls[1, 1]])\n",
    "                            bounding_box[i][1] = np.array([dtls[1, 0], bounding_box[i][0, 1] + dtls[3, 1] ])\n",
    "                            bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                            bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                            \n",
    "                            #print(\"=====================New Bounding Box================\")\n",
    "                            #print_bounding_box(bounding_box[i])\n",
    "                            \n",
    "                    transformed_dict[key] = bounding_box\n",
    "        else:\n",
    "            return data_dict\n",
    "        \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip(object):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        \n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        transformed_dict = dict()\n",
    "        \n",
    "        #print(\"Horizontal Flip Bounding Box\")\n",
    "        \n",
    "        image = data_dict['image']\n",
    "        w, h = image.size\n",
    "        \n",
    "        #print(\"Flipped image dimension : \",w,\"x\",h)\n",
    "        \n",
    "        if random.random() < self.p:\n",
    "            \n",
    "            for key in data_dict.keys():\n",
    "                \n",
    "                if key == 'image':\n",
    "                    image = F.hflip(image)\n",
    "                    \n",
    "                    #image.show()\n",
    "                    \n",
    "                    transformed_dict[key] = image\n",
    "                else:\n",
    "                    bounding_box = data_dict[key]\n",
    "                    \n",
    "                    #print(bounding_box)\n",
    "                    \n",
    "                    for i, dtls in enumerate(bounding_box):\n",
    "                        \n",
    "                        #print(\"=====================Old Bounding Box================\")\n",
    "                        #print_bounding_box(dtls)\n",
    "                        \n",
    "                        if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
    "                            bounding_box[i][0] = np.array([w - (dtls[0, 0] + dtls[3, 0]), dtls[0, 1]])\n",
    "                            bounding_box[i][1] = np.array([bounding_box[i][0, 0] + dtls[3, 0], dtls[1, 1] ])\n",
    "                            bounding_box[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
    "                            bounding_box[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
    "                            \n",
    "                            #print(\"=====================New Bounding Box================\")\n",
    "                            #print_bounding_box(bounding_box[i])\n",
    "                            \n",
    "                    transformed_dict[key] = bounding_box\n",
    "        else:\n",
    "            return data_dict\n",
    "        \n",
    "        return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "    \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, data_dict):\n",
    "        \n",
    "        bounding_box = dict()\n",
    "        \n",
    "        for key in data_dict.keys():\n",
    "            \n",
    "            if key == 'image':\n",
    "                image = data_dict[key]\n",
    "                bounding_box[key] = F.normalize(image, self.mean, self.std)\n",
    "            else:\n",
    "                bounding_box[key] = data_dict[key]\n",
    "                \n",
    "        return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(object):\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert callable(lambd), repr(type(lambd).__name__) + \\\n",
    "            \" object is not callable\"\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorJitter(object):\n",
    "\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = self._check_input(brightness, 'brightness')\n",
    "        self.contrast = self._check_input(contrast, 'contrast')\n",
    "        self.saturation = self._check_input(saturation, 'saturation')\n",
    "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
    "                                     clip_first_on_zero=False)\n",
    "\n",
    "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
    "        if isinstance(value, numbers.Number):\n",
    "            if value < 0:\n",
    "                raise ValueError(\n",
    "                    \"If {} is a single number, it must be non negative.\".format(name))\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\n",
    "                    \"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n",
    "\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness, contrast, saturation, hue):\n",
    "\n",
    "        tforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast is not None:\n",
    "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        if saturation is not None:\n",
    "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
    "            tforms.append(\n",
    "                Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
    "\n",
    "        if hue is not None:\n",
    "            hue_factor = random.uniform(hue[0], hue[1])\n",
    "            tforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
    "\n",
    "        random.shuffle(tforms)\n",
    "        transform = transforms.Compose(tforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        items = dict()\n",
    "        for key in sample.keys():\n",
    "            if key == 'image':\n",
    "                image = sample[key]\n",
    "                transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "                items[key] =  transform(image)\n",
    "            else:\n",
    "                items[key] = sample[key]\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(bound_box):\n",
    "    \n",
    "    xmin = int(bound_box.find('xmin').text)\n",
    "    ymin = int(bound_box.find('ymin').text)\n",
    "    xmax = int(bound_box.find('xmax').text)\n",
    "    ymax = int(bound_box.find('ymax').text)\n",
    "    return np.array((xmin,ymin,xmax,ymax,(xmin+xmax)/2,(ymin+ymax)/2,np.abs(xmax-xmin),np.abs(ymin-ymax))).reshape(-1,2)\n",
    "\n",
    "def scaled_bounding_box(bound_box, scale):\n",
    "    \n",
    "    bound_box[0] = bound_box[0]/scale\n",
    "    bound_box[1] = bound_box[1]/scale\n",
    "    \n",
    "    bound_box[2][0] = (bound_box[0, 0]+bound_box[1, 0]) /2.0\n",
    "    bound_box[2][1] = (bound_box[0, 1]+bound_box[1, 1])/2.0\n",
    "    \n",
    "    bound_box[3][0] = torch.abs(bound_box[0, 0]-bound_box[1, 0])\n",
    "    bound_box[3][1] = torch.abs(bound_box[0, 1]-bound_box[1, 1])\n",
    "    return bound_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(bound_box, img_size):\n",
    "    \n",
    "    bound_box.float()\n",
    "    \n",
    "    height = bound_box[3][0]\n",
    "    width  = bound_box[3][1]\n",
    "    \n",
    "    bound_box = scaled_bounding_box(bound_box, 4.0)\n",
    "    \n",
    "    image_heatmap = torch.zeros(int(img_size[0]/4),int(img_size[1]/4))\n",
    "    \n",
    "    k_size=8\n",
    "    kernel = cv2.getGaussianKernel(k_size, 8)\n",
    "    kernel = np.dot(kernel, kernel.T)\n",
    "    kernel *= 100\n",
    "    \n",
    "    if bound_box[2][1].item()+k_size > image_heatmap.shape[0]-1:\n",
    "        y_begin = image_heatmap.shape[0]-1-k_size         \n",
    "    else:\n",
    "        y_begin = int(bound_box[2][1].item())\n",
    "\n",
    "    if bound_box[2][0].item()+k_size > image_heatmap.shape[1]-1:\n",
    "        x_begin = image_heatmap.shape[1]-1-k_size\n",
    "    else:\n",
    "        x_begin = int(bound_box[2][0].item())\n",
    "        \n",
    "    y_end = y_begin + (k_size)\n",
    "    x_end = x_begin + (k_size)\n",
    "    image_heatmap[y_begin : y_end, x_begin : x_end] = torch.from_numpy(kernel)\n",
    "    return image_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataloader(loader, num_of_image = 1, avDev = torch.device(\"cuda\")):\n",
    "    \n",
    "    k=0\n",
    "    for j,train_data in enumerate(loader):\n",
    "        \n",
    "        images = train_data['image'].to(avDev)\n",
    "        heatmaps = train_data['heatmap'].to(avDev)\n",
    "        plt.figure(figsize=(40,20))\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.title('Train image' + str(k))\n",
    "        plt.imshow(images[0][0].cpu())\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.title('ball')\n",
    "        plt.imshow(heatmaps[0][0].cpu().detach().numpy(), cmap= 'gray')\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.title('goalpost')\n",
    "        plt.imshow(heatmaps[0][1].cpu().detach().numpy(), cmap= 'gray')\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.title('robot')\n",
    "        plt.imshow(heatmaps[0][2].cpu().detach().numpy(), cmap= 'gray')\n",
    "        plt.show()\n",
    "        \n",
    "        k+=1\n",
    "        \n",
    "        if k == num_of_image:\n",
    "            return\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, mode, train_loader, criterion_blob, criterion_seg, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss_blob = 0.0\n",
    "    train_loss_seg = 0.0\n",
    "    train_loss = 0.0\n",
    "    loss_blob = 0\n",
    "    loss_seg = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(tqdm(train_loader)):\n",
    "        if(mode == \"blob\"):\n",
    "            image = sample_batched['image'].to(device)\n",
    "            label = sample_batched['heatmap'].to(device)\n",
    "            output = model(image)\n",
    "            optimizer.zero_grad()\n",
    "            loss_blob = criterion_blob(output, label)\n",
    "            \n",
    "        elif(mode == \"seg\"):\n",
    "            image = sample_batched[0].to(device)\n",
    "            label = sample_batched[1].to(device)\n",
    "            label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "            output = model(image)\n",
    "            optimizer.zero_grad()\n",
    "            loss_seg = criterion_seg(output, label)\n",
    "        \n",
    "        \n",
    "        #print(\"Output Size = \",output.size(),\"  Label Size\", label.size())\n",
    "        train_loss += (loss_blob + loss_seg)\n",
    "        \n",
    "        if(mode == \"blob\"):\n",
    "            loss_blob.backward()\n",
    "            \n",
    "        elif(mode == \"seg\"):\n",
    "            loss_seg.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "          \n",
    "    return train_loss.item(), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_of_shape(image):\n",
    "    \n",
    "    centers = []\n",
    "    \n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    threshold = cv2.threshold(blurred,0.7, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    contours = cv2.findContours(threshold.copy().astype(np.uint8), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = imutils.grab_contours(contours)\n",
    "    \n",
    "    for contour in contours:\n",
    "        \n",
    "        M = cv2.moments(contour)\n",
    "        if(M[\"m00\"]>0):\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centers.append((cX,cY))\n",
    "            \n",
    "    return torch.FloatTensor(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculation(actual_boxes, output_boxes, threshold_distance):\n",
    "    \n",
    "    num_legal_boxes = 0\n",
    "    num_output_box = len(output_boxes)\n",
    "    \n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    \n",
    "    for box_num in range(actual_boxes.size(0)):\n",
    "        \n",
    "        if(actual_boxes[box_num][2][0] > 0 or actual_boxes[box_num][2][0] >0):\n",
    "            num_legal_boxes +=1\n",
    "            \n",
    "    flag_actual_boxes = np.zeros(num_legal_boxes)\n",
    "    \n",
    "    for obox_num in range(num_output_box):\n",
    "        \n",
    "        found = -1\n",
    "        \n",
    "        for abox_num in range(num_legal_boxes):\n",
    "           \n",
    "            a_center = actual_boxes[abox_num][2]/4.0\n",
    "            o_detected = False\n",
    "            o_center = output_boxes[obox_num]\n",
    "            \n",
    "            if(o_center[0] > 0 or o_center[1] > 0):\n",
    "                o_detected = True\n",
    "                \n",
    "            if(o_detected):\n",
    "                dist = distance.euclidean(a_center.cpu(),o_center.cpu())\n",
    "                \n",
    "                if(dist < threshold_distance):\n",
    "                    tp += 1\n",
    "                    found = abox_num\n",
    "                    break\n",
    "        \n",
    "        if(found == -1):\n",
    "            fp +=1\n",
    "        \n",
    "        else:\n",
    "            flag_actual_boxes[found] = 1\n",
    "        \n",
    "    fn = np.count_nonzero(flag_actual_boxes == 0)\n",
    "        \n",
    "    return tp, fn, fp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationAwareConv2d(torch.nn.Conv2d):\n",
    "    \n",
    "    def __init__(self,locationAware,gradient,w,h,in_channels = 256, out_channels=3, kernel_size=1, stride=1, padding=0, bias=True):\n",
    "        \n",
    "        super().__init__(in_channels = 256, out_channels=3, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        if locationAware:\n",
    "            self.locationBias=torch.nn.Parameter(torch.zeros(w,h,3))\n",
    "            self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n",
    "            \n",
    "            if gradient:\n",
    "                for i in range(w):\n",
    "                    self.locationEncode[i,:,1]=self.locationEncode[:,i,0]=i/float(w-1)\n",
    "        \n",
    "        self.up=torch.nn.Upsample(size=(w,h), mode='bilinear', align_corners=False)\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        self.locationAware=locationAware\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        if self.locationAware:\n",
    "            if self.locationBias.device != inputs.device:\n",
    "                self.locationBias=self.locationBias.to(inputs.get_device())\n",
    "                \n",
    "            if self.locationEncode.device != inputs.device:\n",
    "                self.locationEncode=self.locationEncode.to(inputs.get_device())\n",
    "                \n",
    "            b=self.locationBias*self.locationEncode\n",
    "            \n",
    "        convRes=super().forward(inputs)\n",
    "        \n",
    "        if convRes.shape[2]!=self.w and convRes.shape[3]!=self.h:\n",
    "            convRes=self.up(convRes)\n",
    "            \n",
    "        if self.locationAware:\n",
    "            return convRes+b[:,:,0]+b[:,:,1]+b[:,:,2]\n",
    "        \n",
    "        else:\n",
    "            return convRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NimbRoNet2 Model Class\n",
    "\"\"\"\n",
    "\n",
    "class NimbRoNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NimbRoNet2, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        Encoder Block\n",
    "        \"\"\"\n",
    "        self.e_block1 = nn.Sequential(*list(model.children())[0:5])\n",
    "        self.conv_1_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        \n",
    "        self.e_block2 = nn.Sequential(*list(model.children())[5:6])\n",
    "        self.conv_1_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        \n",
    "        self.e_block3 = nn.Sequential(*list(model.children())[6:7])\n",
    "        self.conv_1_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1)\n",
    "        \n",
    "        self.e_block4 = nn.Sequential(*list(model.children())[7:-2])\n",
    "        \"\"\"\n",
    "        Decoder Block\n",
    "        \"\"\"\n",
    "        self.d_block1 = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.ConvTranspose2d(in_channels = 512, out_channels=256, kernel_size=2, stride=2, padding=0, output_padding=0))\n",
    "        \n",
    "        self.d_block2 = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ConvTranspose2d(in_channels = 512, out_channels=256, kernel_size=2, stride=2, padding=0, output_padding=0))\n",
    "        \n",
    "        self.d_block3 = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(512),\n",
    "                        nn.ConvTranspose2d(in_channels = 512, out_channels=128, kernel_size=2, stride=2, padding=0, output_padding=0))\n",
    "        \n",
    "        self.d_block4 = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(256),\n",
    "#                         nn.ConvTranspose2d(in_channels = 256,out_channels=3, kernel_size=1, stride=1, padding=0, output_padding=0))\n",
    "                        LocationAwareConv2d(True, False, 120, 160, in_channels = 256, out_channels=3, kernel_size=1, stride=1, padding=0))\n",
    "        \"\"\"\n",
    "        Location dependent convolution\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        encoder\n",
    "        \"\"\"\n",
    "        #print(\"Input : \",input.shape)\n",
    "        \n",
    "        output = self.e_block1(input)\n",
    "        #print(\"e_block1 : \",output.shape)\n",
    "        \n",
    "        res_1 = self.conv_1_1(output)\n",
    "        #print(\"res_1 : \",res_1.shape)\n",
    "        \n",
    "        output = self.e_block2(output)\n",
    "        #print(\"e_block2 : \",output.shape)\n",
    "        \n",
    "        res_2 = self.conv_1_2(output)\n",
    "        #print(\"res_2 : \",res_2.shape)\n",
    "        \n",
    "        output = self.e_block3(output)\n",
    "        #print(\"e_block3 : \",output.shape)\n",
    "        \n",
    "        res_3 = self.conv_1_3(output)\n",
    "        #print(\"res_3 : \",res_3.shape)\n",
    "        \n",
    "        output = self.e_block4(output)\n",
    "        #print(\"e_block4 : \",output.shape)\n",
    "        \"\"\"\n",
    "        decoder\n",
    "        \"\"\"\n",
    "        output = self.d_block1(output)\n",
    "        #print(\"d_block1 : \",output.shape)\n",
    "        \n",
    "        output = torch.cat((output, res_3), 1)\n",
    "        #print(\"d_block1 + res3 : \",output.shape)\n",
    "        \n",
    "        output = self.d_block2(output)\n",
    "        #print(\"d_block2 : \",output.shape)\n",
    "        \n",
    "        output = torch.cat((output, res_2), 1)\n",
    "        #print(\"d_block2 + res2 : \",output.shape)        \n",
    "        \n",
    "        output = self.d_block3(output)\n",
    "        #print(\"d_block3 : \",output.shape)\n",
    "        \n",
    "        output = torch.cat((output, res_1), 1)\n",
    "        #print(\"d_block3 + res1 : \",output.shape)\n",
    "        \n",
    "        output = self.d_block4(output)\n",
    "        #print(\"d_block4 : \",output.shape)\n",
    "        \"\"\"\n",
    "        Location dependent convolution\n",
    "        \"\"\"\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  cuda\n",
      "Python Version :  3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "Pytorch Version :  1.3.1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    avDev = torch.device(\"cuda\")\n",
    "else:\n",
    "    avDev = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "print(\"Device : \",avDev)\n",
    "print(\"Python Version : \",sys.version)\n",
    "print(\"Pytorch Version : \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob Train Dataset :  3598\n",
      "Segmentation Train Dataset :  1108\n"
     ]
    }
   ],
   "source": [
    "train_transforms = [Rescale((480,640)),\n",
    "              RandomVerticalFlip(),\n",
    "              RandomHorizontalFlip(),\n",
    "              ColorJitter(brightness=0.5, \n",
    "                          contrast=0.5, \n",
    "                          saturation=0.5,\n",
    "                          hue = 0.5),\n",
    "              ToTensor(), \n",
    "              Normalize([0.485, 0.456, 0.406], \n",
    "                        [0.229, 0.224, 0.225])]\n",
    "\n",
    "train_dataset = BlobTrainDataset(path = '../Project/data/blob/forceTrain',transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#print_dataloader(train_loader, num_of_image= 3, avDev = avDev)\n",
    "\n",
    "print(\"Blob Train Dataset : \",len(train_dataset))\n",
    "\n",
    "seg_train_dataset = SegDataset(path = '../Project/data/segmentation/dataset', transform= None)\n",
    "seg_train_loader = torch.utils.data.DataLoader(dataset=seg_train_dataset, \n",
    "                                           batch_size=5, \n",
    "                                           shuffle=True)\n",
    "\n",
    "print(\"Segmentation Train Dataset : \",len(seg_train_dataset))\n",
    "\n",
    "# for i, test_data in enumerate(seg_train_loader):\n",
    "\n",
    "#     plt.figure(figsize=(50,25))\n",
    "#     print(test_data[0].size())\n",
    "#     plt.imshow(torchvision.utils.make_grid(test_data[0], nrow=5).permute(1, 2, 0))\n",
    "#     plt.show()\n",
    "#     plt.figure(figsize=(50,25))\n",
    "# #     plt.imshow(torchvision.utils.make_grid(test_data[1], nrow=1).permute(1, 2, 0))\n",
    "#     print(test_data[1].size())\n",
    "#     plt.imshow(torchvision.utils.make_grid(test_data[1][0][2], nrow=5).permute(1,2,0))\n",
    "#     plt.show()\n",
    "# #     print(np.array(test_data[1][0]))\n",
    "# #     raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NimbRoNet2()\n",
    "model.to(avDev)\n",
    "\n",
    "criterion_blob = nn.MSELoss()\n",
    "criterion_seg = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_blob.to(avDev)\n",
    "criterion_seg.to(avDev)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "                        {\"params\":model.e_block1.parameters(), \"lr\": 0.000001},\n",
    "                        {\"params\":model.e_block2.parameters(), \"lr\": 0.000001},\n",
    "                        {\"params\":model.e_block3.parameters(), \"lr\": 0.000001},\n",
    "                        {\"params\":model.e_block4.parameters(), \"lr\": 0.000001},\n",
    "                        {\"params\":model.d_block1.parameters()},\n",
    "                        {\"params\":model.d_block2.parameters()},\n",
    "                        {\"params\":model.d_block3.parameters()},\n",
    "                        {\"params\":model.d_block4.parameters()},\n",
    "                        {\"params\":model.conv_1_1.parameters()},\n",
    "                        {\"params\":model.conv_1_2.parameters()},\n",
    "                        {\"params\":model.conv_1_3.parameters()}], lr=0.001)\n",
    "\n",
    "# checkpoint = torch.load(\"../model.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# no_epoch = checkpoint['epoch']\n",
    "# print(\"No of epoch : \",no_epoch)\n",
    "# train_loss_1 = checkpoint['train_loss1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abishek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [5, 3, 120, 160]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c05174e3e053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_blob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavDev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1ef338d683a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, mode, train_loader, criterion_blob, criterion_seg, optimizer, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_seg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [5, 3, 120, 160]"
     ]
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "loss_details = [None]*100\n",
    "train_loss = 0\n",
    "for epoch in range(6):\n",
    "\n",
    "    print(\"epoch\",epoch)\n",
    "    train_loss, model = train_model(model, \"seg\", seg_train_loader, criterion_blob, criterion_seg, optimizer, avDev)\n",
    "    loss_details[epoch] = train_loss\n",
    "    print(\"train loss\",train_loss)\n",
    "    total_loss += train_loss\n",
    "    #print(\"total loss\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_test_dataset = SegDataset(path = '../Project/data/segmentation/forceTrain', transform= None)\n",
    "seg_test_loader = torch.utils.data.DataLoader(dataset=seg_test_dataset, \n",
    "                                           batch_size=5, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "for i, test_data in enumerate(seg_test_loader):\n",
    "    if(i <= 10):\n",
    "        images = test_data[0].to(avDev)\n",
    "        target = test_data[1].to(avDev)\n",
    "\n",
    "        output = model(images)\n",
    "        \n",
    "        plt.figure(figsize=(50,25))\n",
    "        plt.imshow(torchvision.utils.make_grid(output.cpu().detach(), nrow=5).permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        print(output.size())\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
